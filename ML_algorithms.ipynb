{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "# import scikit-learn for machine learning\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import xgboost as xgb\n",
    "\n",
    "# GridSearchCV: to find the best hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# import scikit-learn for machine learning\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# packages for data standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# to remove unnecessary warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "# Help: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "def linear_regressor(x, y, fit_intercept, normalize, accuracy_metric, cv=5):\n",
    "    \"\"\" This function performs Linear regression\n",
    "    Args:\n",
    "        x (DataFrame): training feature variables\n",
    "        y (DataFrame): training target variable\n",
    "        fit_intercept (bool): Whether to calculate the intercept for this model\n",
    "        normalize (bool): the regressors will be normalized before regression by subtracting the mean and dividing by the l2-norm\n",
    "        accuracy_metric: accuracy metric to compare the cross-validation splits\n",
    "        cv (int): value of k in k-fold cross-validation\n",
    "    Returns:\n",
    "        grid_result: after grid search with hyper-parameter tuning and cross-validation\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    # instantiate the Linear Regression model\n",
    "    # random state (int): Controls the randomness of the estimator for reproducibility\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Hyperparameters\n",
    "    parameters = {'fit_intercept': fit_intercept,\n",
    "                  'normalize': normalize}\n",
    "\n",
    "    # GridSearchCV: to find the best hyperparameters based on the scoring method\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring=accuracy_metric, cv=cv, n_jobs=-1)\n",
    "\n",
    "    # fit the model with the best hyperparameters\n",
    "    grid_result = grid_search.fit(x, y)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"time = \", (end-start)/60)\n",
    "\n",
    "    # return the model to proceed into prediction\n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "# Help: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
    "\n",
    "def elasticnet_regressor(x, y, alpha, l1_ratio, fit_intercept, normalize, accuracy_metric, cv=5):\n",
    "    \"\"\" This function performs Linear regression\n",
    "    Args:\n",
    "        x (DataFrame): training feature variables\n",
    "        y (DataFrame): training target variable\n",
    "        alpha (float): \n",
    "        l1_ratio (float):\n",
    "        fit_intercept (bool): Whether to calculate the intercept for this model\n",
    "        normalize (bool): the regressors will be normalized before regression by subtracting the mean and dividing by the l2-norm\n",
    "        accuracy_metric: accuracy metric to compare the cross-validation splits\n",
    "        cv (int): value of k in k-fold cross-validation\n",
    "    Returns:\n",
    "        grid_result: after grid search with hyper-parameter tuning and cross-validation\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    # instantiate the Linear Regression model\n",
    "    # random state (int): Controls the randomness of the estimator for reproducibility\n",
    "    model = ElasticNet(random_state=42)\n",
    "\n",
    "    # Hyperparameters\n",
    "    parameters = {'alpha':alpha,\n",
    "                  'l1_ratio':l1_ratio,\n",
    "                  'fit_intercept': fit_intercept,\n",
    "                  'normalize': normalize}\n",
    "\n",
    "    # GridSearchCV: to find the best hyperparameters based on the scoring method\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring=accuracy_metric, cv=cv, n_jobs=-1)\n",
    "\n",
    "    # fit the model with the best hyperparameters\n",
    "    grid_result = grid_search.fit(x, y)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"time = \", (end-start)/60)\n",
    "\n",
    "    # return the model to proceed into prediction\n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_regressor(x, y, base_estimator, n_estimators, learning_rate, loss, accuracy_metric, cv=5):\n",
    "    \"\"\" This function performs AdaBoost regression\n",
    "    Args:\n",
    "        x (DataFrame): training feature variables\n",
    "        y (DataFrame): training target variable\n",
    "        n_estimators (int): The maximum number of estimators at which boosting is terminated\n",
    "        learning_rate (float): Learning rate shrinks the contribution of each regressor\n",
    "        loss (str): The loss function to use when updating the weights after each boosting iteration\n",
    "        accuracy_metric: accuracy metric to compare the cross-validation splits\n",
    "        cv (int): value of k in k-fold cross-validation\n",
    "    Returns:\n",
    "        grid_result: after grid search with hyper-parameter tuning and cross-validation\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    # instantiate the Decision Tree regressor model\n",
    "    # random state (int): Controls the randomness of the estimator for reproducibility\n",
    "    model = AdaBoostRegressor(random_state=42)\n",
    "\n",
    "    # Hyperparameters\n",
    "    parameters = {'base_estimator':base_estimator,\n",
    "                  'n_estimators': n_estimators,\n",
    "                  'learning_rate': learning_rate,\n",
    "                  'loss': loss}\n",
    "\n",
    "    # GridSearchCV: to find the best hyperparameters based on the scoring method\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring=accuracy_metric, cv=cv, n_jobs=-1)\n",
    "\n",
    "    # fit the model with the best hyperparameters\n",
    "    grid_result = grid_search.fit(x, y)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"time = \", (end-start)/60)\n",
    "\n",
    "    # return the model to proceed into prediction\n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regressor\n",
    "# Help: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "\n",
    "def decision_tree_regressor(x, y, criterion, max_features, max_depth, min_samples_leaf, accuracy_metric, cv=5):\n",
    "    \"\"\" This function performs Decision Tree regression\n",
    "    Args:\n",
    "        x (DataFrame): training feature variables\n",
    "        y (DataFrame): training target variable\n",
    "        criterion (str): The function to measure the quality of a split\n",
    "        max_features (str): The number of features to consider when looking for the best split\n",
    "        max_depth (int): The maximum depth of the tree\n",
    "        min_samples_leaf (int): The minimum number of samples required to be at a leaf node\n",
    "        accuracy_metric: accuracy metric to compare the cross-validation splits\n",
    "        cv (int): value of k in k-fold cross-validation\n",
    "    Returns:\n",
    "        grid_result: after grid search with hyper-parameter tuning and cross-validation\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    # instantiate the Decision Tree regressor model\n",
    "    # random state (int): Controls the randomness of the estimator for reproducibility\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "    # Hyperparameters\n",
    "    parameters = {'criterion': criterion,\n",
    "                  'max_features': max_features,\n",
    "                  'max_depth': max_depth,\n",
    "                  'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "    # GridSearchCV: to find the best hyperparameters based on the scoring method\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring=accuracy_metric, cv=cv, n_jobs=-1)\n",
    "\n",
    "    # fit the model with the best hyperparameters\n",
    "    grid_result = grid_search.fit(x, y)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"time = \", (end-start)/60)\n",
    "\n",
    "    # return the model to proceed into prediction\n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "def random_forest_regressor(x, y, criterion, n_estimators, bootstrap, max_features,\n",
    "                            max_depth, min_samples_leaf, accuracy_metric, cv=5):\n",
    "    \"\"\" This function performs Random Forest regression\n",
    "    Args:\n",
    "        x (DataFrame): training feature variables\n",
    "        y (DataFrame): training target variable\n",
    "        criterion : The function to measure the quality of a split\n",
    "        n_estimators (int): The number of trees in the forest\n",
    "        bootstrap (bool): Whether bootstrap samples are used when building trees.\n",
    "        max_features: The number of features to consider when looking for the best split\n",
    "        max_depth (int): The maximum depth of the tree\n",
    "        min_samples_leaf (int): The minimum number of samples required to split an internal node\n",
    "        accuracy_metric: accuracy metric to compare the cross-validation splits\n",
    "        cv (int): value of k in k-fold cross-validation\n",
    "    Returns:\n",
    "        grid_result: after grid search with hyper-parameter tuning and cross-validation\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    # instantiate the Random Forest regressor model\n",
    "    # random state (int): Controls the randomness of the estimator for reproducibility\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    # Hyperparameters\n",
    "    parameters = {'criterion': criterion,\n",
    "                  'n_estimators': n_estimators,\n",
    "                  'bootstrap': bootstrap,\n",
    "                  'max_features': max_features,\n",
    "                  'max_depth': max_depth,\n",
    "                  'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "    # GridSearchCV: to find the best hyperparameters\n",
    "    # based on the scoring method\n",
    "    # optional: 'neg_root_mean_squared_error'\n",
    "    # cv : cross-validation\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring=accuracy_metric, cv=cv, n_jobs=-1)\n",
    "\n",
    "    # fit the model with the best hyperparameters\n",
    "    grid_result = grid_search.fit(x, y)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"time = \", (end-start)/60)\n",
    "\n",
    "    # return the model to proceed into prediction\n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "\n",
    "def gradient_boosting_regressor(x, y, criterion, max_depth, n_estimators, learning_rate, accuracy_metric, cv=5):\n",
    "    \"\"\" This function performs Gradient Boosting regression\n",
    "    Args:\n",
    "        x (DataFrame) : training feature variables\n",
    "        y (DataFrame) : training target variable\n",
    "        criterion: {'friedman_mse', mae', 'mse'}, The function is to measure the quality of a split\n",
    "        n_estimators (int) : The number of boosting stages to perform\n",
    "        max_depth (int) : Maximum depth of the individual regression estimators\n",
    "        learning rate (float) : It shrinks the contribution of each tree by learning_rate\n",
    "        accuracy_metric: accuracy metric to compare the cross-validation splits\n",
    "        cv (int): value of k in k-fold cross-validation\n",
    "    Returns:\n",
    "        grid_result: after grid search with hyper-parameter tuning and cross-validation\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    # instantiate the Gradient Boosting regressor model\n",
    "    # random state (int): Controls the randomness of the estimator for reproducibility\n",
    "    model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "    # Hyperparameters\n",
    "    parameters = {'criterion': criterion,\n",
    "                  'max_depth': max_depth,\n",
    "                  'n_estimators': n_estimators,\n",
    "                  'learning_rate': learning_rate}\n",
    "\n",
    "    # GridSearchCV: to find the best hyperparameters based on the scoring method\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring=accuracy_metric, cv=cv, n_jobs=-1)\n",
    "\n",
    "    # fit the model with the best hyperparameters\n",
    "    grid_result = grid_search.fit(x, y)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"time = \", (end-start)/60)\n",
    "\n",
    "    # return the model to proceed into prediction\n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "\n",
    "def X_gradient_boosting_regressor(x, y, max_depth, n_estimators, learning_rate, accuracy_metric, cv=5):\n",
    "    \"\"\" This function performs Gradient Boosting regression\n",
    "    Args:\n",
    "        x (DataFrame) : training feature variables\n",
    "        y (DataFrame) : training target variable\n",
    "        objective (str): \n",
    "        n_estimators (int) : The number of boosting stages to perform\n",
    "        max_depth (int) : Maximum depth of the individual regression estimators\n",
    "        learning rate (float) : It shrinks the contribution of each tree by learning_rate\n",
    "        accuracy_metric (str): accuracy metric to compare the cross-validation splits\n",
    "        cv (int): value of k in k-fold cross-validation\n",
    "    Returns:\n",
    "        grid_result: after grid search with hyper-parameter tuning and cross-validation\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    # instantiate the Gradient Boosting regressor model\n",
    "    # random state (int): Controls the randomness of the estimator for reproducibility\n",
    "    model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "    # Hyperparameters\n",
    "    parameters = {'max_depth': max_depth,\n",
    "                  'n_estimators': n_estimators,\n",
    "                  'learning_rate': learning_rate}\n",
    "\n",
    "    # GridSearchCV: to find the best hyperparameters based on the scoring method\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring=accuracy_metric, cv=cv, n_jobs=-1)\n",
    "\n",
    "    # fit the model with the best hyperparameters\n",
    "    grid_result = grid_search.fit(x, y)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"time = \", (end-start)/60)\n",
    "\n",
    "    # return the model to proceed into prediction\n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_pipeline = Pipeline([(\"scaler\", StandardScaler()),(\"model\", GradientBoostingRegressor(random_state=42))])\n",
    "\n",
    "# # gbm_param_grid = {'xgb_model__subsample': np.arange(.05, 1, .05),\n",
    "# #                   'xgb_model__max_depth': np.arange(3,20,1),\n",
    "# #                   'xgb_model__colsample_bytree': np.arange(.1,1.05,.05) }\n",
    "\n",
    "# gbm_param_grid = {'model__max_depth': np.arange(6,7,1).tolist(),\n",
    "#                   'model__n_estimators': np.arange(300,301,1).tolist(),\n",
    "#                   'model__criterion': ['mse'], \n",
    "#                   'model__learning_rate': [0.1]}\n",
    "\n",
    "# randomized_neg_mse = GridSearchCV(estimator=xgb_pipeline,\n",
    "#                                         param_grid=gbm_param_grid,\n",
    "#                                         scoring='neg_mean_squared_error', cv=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
